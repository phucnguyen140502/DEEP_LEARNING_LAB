{"cells":[{"cell_type":"markdown","metadata":{"id":"yEW0ES1qvnRz"},"source":["# Problem Setting"]},{"cell_type":"markdown","metadata":{"id":"h389W76kvnRz"},"source":["Suppose you have a neuron model similar to ADALINE (discussed in class) but the activation function (which is an identity function in ADALINE) is replaced by a non-linear activation function. The figure below shows an arbitrary single-layer neuron.\n","\n","- If $\\sigma(z) = z$, the activation function is an identity function and the neuron represents ADALINE.\n","- If $\\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$, the activation function is a hyperbolic tangent (tanh), which is a non-linear sigmoid function (https://en.wikipedia.org/wiki/Sigmoid_function).\n"]},{"cell_type":"markdown","metadata":{"id":"8DS_OLB6vnRz"},"source":["![](images/neuron.png)"]},{"cell_type":"markdown","metadata":{"id":"jQdT5j1xvnR0"},"source":["As mentioned above, in this lab, we are working with a tanh activation function, which we defined as\n","$$\n","\\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}},\n","$$"]},{"cell_type":"markdown","metadata":{"id":"jywj7tH2vnR0"},"source":["![](images/tanh.png)"]},{"cell_type":"markdown","metadata":{"id":"IivIasQAvnR1"},"source":["Here $z$ denotes the net input,  $z = \\mathbf{w}^\\top \\mathbf{x} + b$ (for a single training example, we write $z^{[i]} = \\mathbf{w}^\\top \\mathbf{x}^{[i]} + b$).\n","\n","Assume now that we want learn the parameters of the neuron model for a binary classification task with class labels $y \\in \\{0, 1\\}$ similar to ADALINE. We use the same loss function, mean squared error (MSE), as in ADALINE, during training:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nSBiva8fvnR1"},"source":["$$\n","\\mathbb{L}(\\mathbf{\\hat{y}}, \\mathbf{y}) = \\frac{1}{n} \\sum_{i}^{} (\\hat{y}^{[i]} - y^{[i]})^2.\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Y67Bfa22vnR1"},"source":["## TASK 1: Compute the Loss Gradients with respect to the weights and bias unit"]},{"cell_type":"markdown","metadata":{"id":"FyIIcnVKvnR1"},"source":["Your task is to derive the gradient of the loss $\\mathbb{L}$ with respect to the weight vector and the bias unit and formulate the learning rule.\n","\n","Remember that the gradient of the loss is defined as\n","\n","$$\n","\\nabla_\\mathbf{w} \\mathbb{L}(\\mathbf{w}) =\n","\\begin{bmatrix}\n","\\frac{\\partial \\mathbb{L}(\\mathbf{w})}{\\partial w_1}\\\\\n","\\vdots \\\\\n","\\frac{\\partial \\mathbb{L}(\\mathbf{w})}{\\partial w_m}\\\\\n","\\end{bmatrix}\n","$$\n","\n","**For simplicity, it is sufficient if you write down the partial derivative and learning rule for a single weight $w_j$ and the bias unit $b$**.\n","\n","To provide you with a hint, recall that we computed the partial Loss derivatives for ADALINE as follows:"]},{"cell_type":"markdown","metadata":{"id":"a4QbWhBuvnR2"},"source":["$$\n","\\begin{align}\n","\\frac{\\partial \\mathbb{L}}{\\partial w_j} &= \\frac{\\partial}{\\partial w_j} \\frac{1}{n} \\sum_i (\\hat{y}^{[i]} - y^{[i]} )^2\\\\\n","&= \\frac{\\partial}{\\partial w_j}  \\frac{1}{n} \\sum_i (\\sigma(\\mathbf{w}^T\\mathbf{x}^{[i]} + b) - y^{[i]})^2\\\\\n","\\\\\n","&= \\quad ... \\\\\n","\\\\\n","&= \\sum_i \\frac{2}{n}  (\\sigma(\\mathbf{w}^T\\mathbf{x}^{[i]} + b) - y^{[i]})   x_j^{[i]}\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"MYG03vYxvnR2"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"UuSDy-vDvnR2"},"source":["$$\n","\\begin{align}\n","\\frac{\\partial \\mathbb{L}}{\\partial b} &= \\frac{\\partial}{\\partial b} \\frac{1}{n}\\sum_i (\\hat{y}^{[i]} - y^{[i]} )^2\\\\\n","&= \\frac{\\partial}{\\partial b}  \\frac{1}{n} \\sum_i (\\sigma(\\mathbf{w}^T\\mathbf{x}^{[i]} + b) - y^{[i]})^2\\\\\n","\\\\\n","&= \\quad ... \\\\\n","\\\\\n","&= \\sum_i \\frac{2}{n}  (\\sigma(\\mathbf{w}^T\\mathbf{x}^{[i]} + b) - y^{[i]})\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"VHfr_hdhvnR2"},"source":["Again, your task is to derive\n","\n","- a) $$\\frac{\\partial \\mathbb{L}}{\\partial w_j}$$\n","\n","and\n","\n","- b) $$\\frac{\\partial \\mathbb{L}}{\\partial b}$$\n","\n","for the neuron model with the non-linear activation function\n","\n","$$\n","\\sigma(\\mathbf{w}^\\top \\mathbf{x} + b) =  \\frac{e^{\\mathbf{w}^\\top \\mathbf{x} + b} - e^{- (\\mathbf{w}^\\top \\mathbf{x} + b)}}{e^{\\mathbf{w}^\\top \\mathbf{x}+b} + e^{- (\\mathbf{w}^\\top \\mathbf{x}+ b)}}\n","$$\n","\n","For partial credits in case of a wrong solution, also write down the individual steps in the cell below."]},{"cell_type":"markdown","metadata":{"id":"LLxvYO-ivnR2"},"source":["**!!!Fill in with your solution below!!!**\n","\n","$$\n","\\begin{align}\n","\\frac{\\partial \\mathbb{L}}{\\partial w_j} &= \\frac{\\partial}{\\partial w_j} \\frac{1}{n}\\sum_i (\\hat{y}^{[i]} - y^{[i]} )^2\\\\\n","&= ...\\\\\n","&= ...\\\\\n","&= ...\\\\\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"P0YcrC1xvnR2"},"source":["<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"txFGG_sfvnR3"},"source":["**!!!Fill in with your solution below!!!**\n","$$\n","\\begin{align}\n","\\frac{\\partial \\mathbb{L}}{\\partial b} &= \\frac{\\partial}{\\partial b} \\frac{1}{n} \\sum_i (\\hat{y}^{[i]} - y^{[i]} )^2\\\\\n","&= ...\\\\\n","&= ...\\\\\n","&= ...\\\\\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"u8EwytlGvnR3"},"source":["# Coding Section"]},{"cell_type":"markdown","metadata":{"id":"RJ0f88xfvnR3"},"source":["Complete the code below, the missing parts are indicated via\n","\n","    # <YOUR CODE HERE>\n","    \n","to implement the neuron model (it is very similar to the ADALINE model we discussed in class, except the derivatives)."]},{"cell_type":"markdown","metadata":{"id":"QQ1DhQVRvnR3"},"source":["## Imports (Don't modify this section, just execute)"]},{"cell_type":"markdown","metadata":{"id":"V-pJfh7rvnR3"},"source":["**No modification required.** You should execute this code and are encouraged to explore it further, but it is recommended to  not make any alterations here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Ewzc_QrvnR3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import torch\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"KH3dNLwCvnR4"},"source":["## Loading the dataset (Don't modify this section, just execute)"]},{"cell_type":"markdown","metadata":{"id":"AyydeL-kvnR4"},"source":["**No modification required.** You should execute this code and are encouraged to explore it further, but it is recommended to  not make any alterations here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3yLH83JvnR4"},"outputs":[],"source":["df = pd.read_csv('./datasets/iris.data', index_col=None, header=None)\n","df.columns = ['x1', 'x2', 'x3', 'x4', 'y']\n","df = df.iloc[50:150]\n","df['y'] = df['y'].apply(lambda x: 0 if x == 'Iris-versicolor' else 1)\n","df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOdQ_3BnvnR4"},"outputs":[],"source":["# Assign features and target\n","\n","X = torch.tensor(df[['x2', 'x3']].values, dtype=torch.float)\n","y = torch.tensor(df['y'].values, dtype=torch.int)\n","\n","# Shuffling & train/test split\n","\n","torch.manual_seed(123)\n","shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n","\n","X, y = X[shuffle_idx], y[shuffle_idx]\n","\n","percent70 = int(shuffle_idx.size(0)*0.7)\n","\n","X_train, X_test = X[shuffle_idx[:percent70]], X[shuffle_idx[percent70:]]\n","y_train, y_test = y[shuffle_idx[:percent70]], y[shuffle_idx[percent70:]]\n","\n","# Normalize (mean zero, unit variance)\n","\n","mu, sigma = X_train.mean(dim=0), X_train.std(dim=0)\n","X_train = (X_train - mu) / sigma\n","X_test = (X_test - mu) / sigma"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ihYGyVWvnR4"},"outputs":[],"source":["plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], label='class 0')\n","plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], label='class 1')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjTC0OoIvnR4"},"outputs":[],"source":["plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], label='class 0')\n","plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], label='class 1')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WyDaE_EhvnR4"},"source":["## TASK 2: Implement the Neuron Model (Modifications required) -- 25 pts"]},{"cell_type":"markdown","metadata":{"id":"AhpIrj1RvnR4"},"source":["Your task is to complete the `backward` method to compute the gradients based on the gradients you computed in TASK1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08ebL4z8vnR5"},"outputs":[],"source":["class NeuronModel():\n","    def __init__(self, num_features):\n","        self.num_features = num_features\n","        self.weights = torch.zeros(num_features, 1,\n","                                   dtype=torch.float)\n","        self.bias = torch.zeros(1, dtype=torch.float)\n","\n","    def activation_func(self, x):\n","        return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n","\n","    def netinput_func(self, x, w, b):\n","         return torch.add(torch.mm(x, w), b)\n","\n","    def forward(self, x):\n","        netinputs = self.netinput_func(x, self.weights, self.bias)\n","        activations = self.activation_func(netinputs)\n","        return activations.view(-1)\n","\n","    def backward(self, x, yhat, y):\n","\n","        # note that here, \"yhat\" are the \"activations\"\n","        netinputs = self.netinput_func(x, self.weights, self.bias)\n","\n","        ###############################################################################\n","        # YOU ONLY NEED TO EDIT IN THE BOX BELOW\n","        ###############################################################################\n","        grad_loss_yhat = # < YOUR CODE>\n","        grad_yhat_bias = # < YOUR CODE>\n","        grad_yhat_weights = # < YOUR CODE>\n","\n","        grad_loss_weights = # < YOUR CODE> based on grad_yhat_weights & grad_loss_yhat\n","\n","        grad_loss_bias = # < YOUR CODE> based on grad_yhat_weights & grad_loss_yhat\n","        ################################################################################\n","\n","        return (-1)*grad_loss_weights, (-1)*grad_loss_bias"]},{"cell_type":"markdown","metadata":{"id":"eGkFflLgvnR5"},"source":["# No modifications required beyond this point"]},{"cell_type":"markdown","metadata":{"id":"RLoAjll-vnR5"},"source":["You do not need to modify anything below. However, you should run and analyze the code to verify that your implementation of the Neuron model is likely correct."]},{"cell_type":"markdown","metadata":{"id":"ZtQ_0gENvnR5"},"source":["## Training the Neuron Model (Don't modify this section, just execute)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WTItK5LvnR5"},"outputs":[],"source":["####################################################\n","##### Training and evaluation wrappers\n","###################################################\n","\n","def loss(yhat, y):\n","    return torch.mean((yhat - y)**2)\n","\n","\n","def train(model, x, y, num_epochs,\n","          learning_rate=0.01, seed=123, minibatch_size=10):\n","    cost = []\n","\n","    torch.manual_seed(seed)\n","    for e in range(num_epochs):\n","\n","        #### Shuffle epoch\n","        shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n","        minibatches = torch.split(shuffle_idx, minibatch_size)\n","\n","        for minibatch_idx in minibatches:\n","\n","            #### Compute outputs ####\n","            yhat = model.forward(x[minibatch_idx])\n","\n","            #### Compute gradients ####\n","            negative_grad_w, negative_grad_b = \\\n","                model.backward(x[minibatch_idx], yhat, y[minibatch_idx])\n","\n","            #### Update weights ####\n","            model.weights += learning_rate * negative_grad_w\n","            model.bias += learning_rate * negative_grad_b\n","\n","            #### Logging ####\n","            #minibatch_loss = loss(yhat, y[minibatch_idx])\n","            #print('    Minibatch MSE: %.3f' % minibatch_loss)\n","\n","        #### Logging ####\n","        yhat = model.forward(x)\n","        curr_loss = loss(yhat, y)\n","        print('Epoch: %03d' % (e+1), end=\"\")\n","        print(' | MSE: %.5f' % curr_loss)\n","        cost.append(curr_loss)\n","\n","    return cost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uv-ogsxNvnR6"},"outputs":[],"source":["model = NeuronModel(num_features=X_train.size(1))\n","cost = train(model,\n","             X_train, y_train.float(),\n","             num_epochs=150,\n","             learning_rate=0.2,\n","             seed=123,\n","             minibatch_size=10)"]},{"cell_type":"markdown","metadata":{"id":"TVEgkbxMvnR6"},"source":["## Evaluate the Trained Model  (Don't modify this section, just execute)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcMMqN7PvnSB"},"outputs":[],"source":["plt.plot(range(len(cost)), cost)\n","plt.ylabel('Mean Squared Error')\n","plt.xlabel('Epoch')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9twtXajlvnSB"},"outputs":[],"source":["print('Weights', model.weights)\n","print('Bias', model.bias)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmCNSJOOvnSC"},"outputs":[],"source":["train_pred = model.forward(X_train)\n","train_acc = torch.mean(\n","    (torch.where(train_pred > 0.5,\n","                 torch.tensor(1),\n","                 torch.tensor(0)).int() == y_train).float())\n","\n","test_pred = model.forward(X_test)\n","test_acc = torch.mean(\n","    (torch.where(train_pred > 0.5,\n","                 torch.tensor(1),\n","                 torch.tensor(0)).int() == y_train).float())\n","\n","print('Training Accuracy: %.2f%%' % (train_acc*100))\n","print('Test Accuracy: %.2f%%' % (test_acc*100))"]},{"cell_type":"markdown","metadata":{"id":"KH4OgVJhvnSC"},"source":["## Decision Boundary  (Don't modify this section, just execute)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"piyBdMFCvnSC"},"outputs":[],"source":["##########################\n","### 2D Decision Boundary\n","##########################\n","\n","w, b = model.weights, model.bias\n","\n","x_min = -3\n","y_min = ( (-(w[0] * x_min) - b[0])\n","          / w[1] )\n","\n","x_max = 3\n","y_max = ( (-(w[0] * x_max) - b[0])\n","          / w[1] )\n","\n","\n","fig, ax = plt.subplots(1, 2, sharex=True, figsize=(7, 3))\n","\n","ax[0].plot([x_min, x_max], [y_min, y_max])\n","ax[1].plot([x_min, x_max], [y_min, y_max])\n","\n","ax[0].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], label='class 0', marker='o')\n","ax[0].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], label='class 1', marker='s')\n","\n","ax[1].scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], label='class 0', marker='o')\n","ax[1].scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], label='class 1', marker='s')\n","\n","ax[1].legend(loc='upper left')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}